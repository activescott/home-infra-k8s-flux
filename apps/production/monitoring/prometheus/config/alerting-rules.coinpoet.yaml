# https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
groups:
  - name: gpupoet
    #labels:
    #  team: scott
    rules:
      - alert: ProbeFailure
        expr: up{instance="gpupoet.com"} < 1.0
        # Alerting rules without the for clause will become active on the first evaluation.
        # for 60m makes it so two consecutive scrapes have to fail
        for: 60m
        # keep_firing_for clause tells Prometheus to keep this alert firing for the specified duration after the firing condition was last met. Alerting rules without the keep_firing_for clause will deactivate on the first evaluation where the condition is not met (assuming any optional for duration described above has been satisfied).
        # it seems since we scrape very rarely we have to increase this value or it will resolve in 5m.
        #keep_firing_for: 35m
        labels:
          severity: critical
        annotations:
          summary: Probe failure. Check https://gpupoet.com/ops/metrics

      - alert: MissingMetricsFailure
        # we're getting back 7 metrics (samples) in each scrape. sometimes vercel timeouts cause prometheus to assume a HTTP 200 response, but no metrics are received?
        expr: scrape_samples_scraped{instance="gpupoet.com"} < 7.0
        # Alerting rules without the for clause will become active on the first evaluation.
        # for 60m makes it so two consecutive scrapes have to fail
        for: 60m
        # keep_firing_for clause tells Prometheus to keep this alert firing for the specified duration after the firing condition was last met. Alerting rules without the keep_firing_for clause will deactivate on the first evaluation where the condition is not met (assuming any optional for duration described above has been satisfied).
        # it seems since we scrape very rarely we have to increase this value or it will resolve in 5m.
        #keep_firing_for: 35m
        labels:
          severity: critical
        annotations:
          summary: Missing metrics. Check https://gpupoet.com/ops/metrics

      - alert: CacheRevalidationJobFailure
        # Alert when the cache revalidation job fails (coinpoet_last_job_success = 0)
        expr: coinpoet_last_job_success{instance="gpupoet.com"} == 0
        # Wait 5 minutes to avoid false positives during brief metrics collection issues
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Cache revalidation job failed"
          description: "The gpupoet cache revalidation cronjob has failed. Check the app logs and https://gpupoet.com/ops/metrics for details."
