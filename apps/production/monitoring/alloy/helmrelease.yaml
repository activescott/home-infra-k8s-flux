apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: alloy
  namespace: monitoring
spec:
  interval: 10m
  chart:
    spec:
      chart: alloy
      version: "1.5.2"
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: monitoring
      interval: 24h
  values:
    alloy:
      mounts:
        extra:
          - name: plex-logs
            mountPath: /mnt/plex-logs
            readOnly: true
      # Alloy log collection pipeline:
      #
      # 1. discovery.kubernetes - Queries the K8s API to find all running pods.
      #
      # 2. discovery.relabel - Maps verbose internal K8s metadata labels to clean,
      #    short names (e.g. __meta_kubernetes_namespace -> namespace). These become
      #    the label dimensions searchable in Loki/Grafana, enabling queries like
      #    {namespace="gpupoet-prod", app="app"}.
      #    A synthetic "service_name" label is also created by joining
      #    namespace + app (e.g. "tinkerbell-prod/app") for use in Grafana's
      #    service dropdown where bare "app" would be ambiguous.
      #
      # 3. loki.source.kubernetes - Tails container logs from the discovered pods
      #    (via the K8s API, similar to `kubectl logs -f`) and attaches the
      #    relabeled metadata to each log line.
      #
      # 4. loki.process - Processes log lines before sending to Loki:
      #    - Joins multi-line stack traces into single entries
      #    - Drops health check request noise
      #    - Parses pino JSON to extract level and msg
      #    - Falls back msg to the full log line for non-JSON entries
      #    - Maps numeric pino levels to string names (info, warn, error, etc.)
      #    - Promotes level to a Loki label for efficient filtering
      #    - Stores msg as structured metadata for Grafana visibility
      #    - Preserves raw JSON body so all fields are queryable via | json
      #
      # 5. loki.write - Pushes labeled log lines to Loki's HTTP API.
      configMap:
        content: |
          // Stage 1: Discover all pods via the Kubernetes API
          discovery.kubernetes "pods" {
            role = "pod"
          }

          // Stage 2: Relabel raw K8s metadata into clean Loki labels
          discovery.relabel "pods" {
            targets = discovery.kubernetes.pods.targets

            rule {
              source_labels = ["__meta_kubernetes_pod_node_name"]
              target_label  = "__host__"
            }
            rule {
              source_labels = ["__meta_kubernetes_namespace"]
              target_label  = "namespace"
            }
            rule {
              source_labels = ["__meta_kubernetes_pod_name"]
              target_label  = "pod"
            }
            rule {
              source_labels = ["__meta_kubernetes_pod_container_name"]
              target_label  = "container"
            }
            rule {
              source_labels = ["__meta_kubernetes_pod_label_app"]
              target_label  = "app"
            }
            // Build a "service_name" label from namespace/app so that
            // Grafana's service dropdown shows e.g. "tinkerbell-prod/app"
            // instead of a single ambiguous "app" entry.
            rule {
              source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_label_app"]
              separator     = "/"
              target_label  = "service_name"
            }
          }

          // Stage 3: Tail container logs from discovered pods
          loki.source.kubernetes "pods" {
            targets    = discovery.relabel.pods.output
            forward_to = [loki.process.default.receiver]
          }

          // Stage 4: Process log lines (parse JSON, drop noise, extract fields)
          loki.process "default" {
            // Join multi-line stack traces into single log entries.
            // Lines starting with { (JSON) or non-whitespace begin a new entry;
            // indented continuation lines (e.g. "    at Module._compile(...)") are
            // appended to the preceding entry.
            stage.multiline {
              firstline     = "^(\\{|\\S)"
              max_wait_time = "3s"
              max_lines     = 128
            }

            // Drop health check request logs to reduce noise.
            // Covers tinkerbell (/health/*) and gpupoet (/api/health/*).
            stage.drop {
              expression = ".*GET /(api/)?health/(readiness|liveness).*"
            }

            // Extract level and msg from pino JSON for label promotion
            // and structured metadata. Non-JSON lines pass through since
            // stage.json silently skips them.
            stage.json {
              expressions = {
                level = "level",
                msg   = "msg",
              }
            }

            // For non-JSON lines, msg is empty after stage.json. Fall back
            // to the full log line so msg is always populated.
            stage.template {
              source   = "msg"
              template = "{{ "{{" }} if .Value {{ "}}" }}{{ "{{" }} .Value {{ "}}" }}{{ "{{" }} else {{ "}}" }}{{ "{{" }} .Entry {{ "}}" }}{{ "{{" }} end {{ "}}" }}"
            }

            // Pino outputs numeric levels (10=trace, 20=debug, 30=info, etc.)
            // but Loki's detected_level expects string values ("info", "error").
            // Without this mapping, levels show as raw numbers in Grafana and
            // level-based filtering/color-coding doesn't work.
            // Uses stage.template with Helm-escaped curly braces (same
            // technique as the Plex section).
            stage.template {
              source   = "level"
              template = "{{ "{{" }} if eq .Value \"10\" {{ "}}" }}trace{{ "{{" }} else if eq .Value \"20\" {{ "}}" }}debug{{ "{{" }} else if eq .Value \"30\" {{ "}}" }}info{{ "{{" }} else if eq .Value \"40\" {{ "}}" }}warn{{ "{{" }} else if eq .Value \"50\" {{ "}}" }}error{{ "{{" }} else if eq .Value \"60\" {{ "}}" }}fatal{{ "{{" }} else {{ "}}" }}{{ "{{" }} .Value {{ "}}" }}{{ "{{" }} end {{ "}}" }}"
            }

            // Promote level to a Loki label for efficient stream selectors
            // like {level="error"}. Low cardinality (6 values).
            stage.labels {
              values = {
                level = "",
              }
            }

            // Store msg as structured metadata so it's visible in Grafana
            // without needing | json. For JSON lines this is the pino msg
            // field; for non-JSON lines it's the full log line.
            stage.structured_metadata {
              values = {
                msg = "",
              }
            }

            // The raw JSON log body is preserved (no stage.output), so all
            // pino fields (module, noteId, url, err, etc.) remain queryable
            // with | json in LogQL.

            forward_to = [loki.write.default.receiver]
          }

          // Stage 5: Push labeled log lines to Loki
          loki.write "default" {
            endpoint {
              url = "http://loki.monitoring.svc:3100/loki/api/v1/push"
            }
          }

          // ── Plex file-based log collection ──────────────────────────────
          // Plex writes logs to files rather than stdout. The host directory
          // is mounted into Alloy at /mnt/plex-logs via a hostPath volume.
          // DirectoryOrCreate ensures Alloy pods on nodes without the Plex
          // data directory start with an empty dir (no files to tail).

          // Discover *.log files under the mounted Plex log directory
          local.file_match "plex_logs" {
            path_targets = [{
              __path__     = "/mnt/plex-logs/*.log",
              namespace    = "plex",
              app          = "plex",
              source       = "file",
              service_name = "plex/plex",
            }]
          }

          // Tail discovered Plex log files, starting from end to avoid
          // ingesting all historical data on first startup
          loki.source.file "plex_logs" {
            targets    = local.file_match.plex_logs.targets
            forward_to = [loki.process.plex.receiver]

            tail_from_end = true
          }

          // Parse Plex log format: "Jan 27, 2026 12:39:48.421 [tid] LEVEL - msg"
          loki.process "plex" {
            stage.regex {
              expression = "^\\S+ \\d+, \\d{4} [\\d:.]+ \\[\\d+\\] (?P<level>\\S+) - (?P<message>.*)$"
            }

            // Normalize level to lowercase. Uses stage.template because
            // stage.replace doesn't modify extracted values or the log line
            // in this Alloy version. The {{ "{{" }}/{{ "}}" }} escaping
            // prevents Helm's tpl function from interpreting the River
            // template delimiters as Go template actions.
            stage.template {
              source   = "level"
              template = "{{ "{{" }} .Value | lower {{ "}}" }}"
            }

            // Use the parsed message as the log line body
            stage.output {
              source = "message"
            }

            // Promote level to a Loki label
            stage.labels {
              values = {
                level = "",
              }
            }

            // Attach filename as structured metadata so individual log files
            // can be distinguished in LogQL queries
            stage.structured_metadata {
              values = {
                filename = "",
              }
            }

            forward_to = [loki.write.default.receiver]
          }

    controller:
      type: daemonset
      volumes:
        extra:
          - name: plex-logs
            hostPath:
              path: "/mnt/thedatapool/app-data/plex/config/Library/Application Support/Plex Media Server/Logs"
              type: DirectoryOrCreate

    serviceAccount:
      create: true

    rbac:
      create: true

    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
