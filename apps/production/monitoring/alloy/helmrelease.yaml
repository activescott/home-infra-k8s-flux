apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: alloy
  namespace: monitoring
spec:
  interval: 10m
  chart:
    spec:
      chart: alloy
      version: "1.5.2"
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: monitoring
      interval: 24h
  values:
    alloy:
      mounts:
        extra:
          - name: plex-logs
            mountPath: /mnt/plex-logs
            readOnly: true
      # Alloy log collection pipeline:
      #
      # 1. discovery.kubernetes - Queries the K8s API to find all running pods.
      #
      # 2. discovery.relabel - Maps verbose internal K8s metadata labels to clean,
      #    short names (e.g. __meta_kubernetes_namespace -> namespace). These become
      #    the label dimensions searchable in Loki/Grafana, enabling queries like
      #    {namespace="gpupoet-prod", app="app"}.
      #    A synthetic "service_name" label is also created by joining
      #    namespace + app (e.g. "tinkerbell-prod/app") for use in Grafana's
      #    service dropdown where bare "app" would be ambiguous.
      #
      # 3. loki.source.kubernetes - Tails container logs from the discovered pods
      #    (via the K8s API, similar to `kubectl logs -f`) and attaches the
      #    relabeled metadata to each log line.
      #
      # 4. loki.process - Processes log lines before sending to Loki:
      #    - Joins multi-line stack traces into single entries
      #    - Drops health check request noise
      #    - Parses pino JSON to extract level, msg, module
      #    - Maps numeric pino levels to string names (info, warn, error, etc.)
      #    - Promotes level to a Loki label for efficient filtering
      #    - Attaches msg and module as structured metadata
      #
      # 5. loki.write - Pushes labeled log lines to Loki's HTTP API.
      configMap:
        content: |
          // Stage 1: Discover all pods via the Kubernetes API
          discovery.kubernetes "pods" {
            role = "pod"
          }

          // Stage 2: Relabel raw K8s metadata into clean Loki labels
          discovery.relabel "pods" {
            targets = discovery.kubernetes.pods.targets

            rule {
              source_labels = ["__meta_kubernetes_pod_node_name"]
              target_label  = "__host__"
            }
            rule {
              source_labels = ["__meta_kubernetes_namespace"]
              target_label  = "namespace"
            }
            rule {
              source_labels = ["__meta_kubernetes_pod_name"]
              target_label  = "pod"
            }
            rule {
              source_labels = ["__meta_kubernetes_pod_container_name"]
              target_label  = "container"
            }
            rule {
              source_labels = ["__meta_kubernetes_pod_label_app"]
              target_label  = "app"
            }
            // Build a "service_name" label from namespace/app so that
            // Grafana's service dropdown shows e.g. "tinkerbell-prod/app"
            // instead of a single ambiguous "app" entry.
            rule {
              source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_label_app"]
              separator     = "/"
              target_label  = "service_name"
            }
          }

          // Stage 3: Tail container logs from discovered pods
          loki.source.kubernetes "pods" {
            targets    = discovery.relabel.pods.output
            forward_to = [loki.process.default.receiver]
          }

          // Stage 4: Process log lines (parse JSON, drop noise, extract fields)
          loki.process "default" {
            // Join multi-line stack traces into single log entries.
            // Lines starting with { (JSON) or non-whitespace begin a new entry;
            // indented continuation lines (e.g. "    at Module._compile(...)") are
            // appended to the preceding entry.
            stage.multiline {
              firstline     = "^(\\{|\\S)"
              max_wait_time = "3s"
              max_lines     = 128
            }

            // Drop health check request logs to reduce noise.
            // Covers tinkerbell (/health/*) and gpupoet (/api/health/*).
            stage.drop {
              expression = ".*GET /(api/)?health/(readiness|liveness).*"
            }

            // Parse pino JSON log lines. Extracts level, msg, module, time.
            // Non-JSON lines (plain text HTTP logs, CronJob output) pass through
            // unmodified since stage.json silently skips unparseable lines.
            stage.json {
              expressions = {
                level  = "level",
                msg    = "msg",
                module = "module",
                time   = "time",
              }
            }

            // Map pino numeric levels to human-readable names for Grafana
            // level detection and color coding.
            //
            // NOTE: We use stage.replace instead of stage.template here
            // because stage.template uses River's double-curly-brace syntax
            // which conflicts with Helm's Go template processing — the Alloy
            // chart passes configMap.content through Helm's tpl function,
            // which interprets double-curly-braces as Go template actions.
            stage.replace {
              source     = "level"
              expression = "^10$"
              replace    = "trace"
            }
            stage.replace {
              source     = "level"
              expression = "^20$"
              replace    = "debug"
            }
            stage.replace {
              source     = "level"
              expression = "^30$"
              replace    = "info"
            }
            stage.replace {
              source     = "level"
              expression = "^40$"
              replace    = "warn"
            }
            stage.replace {
              source     = "level"
              expression = "^50$"
              replace    = "error"
            }
            stage.replace {
              source     = "level"
              expression = "^60$"
              replace    = "fatal"
            }

            // Replace the log line body with just the msg field from the
            // parsed JSON. This makes the Grafana log view show clean
            // human-readable messages instead of raw JSON. The full JSON
            // fields (level, time, module, etc.) are still available as
            // structured metadata and labels.
            // For non-JSON lines (plain HTTP logs, CronJob output), stage.json
            // doesn't populate the extracted map, so stage.output is a no-op
            // and the original log line passes through unchanged.
            stage.output {
              source = "msg"
            }

            // Promote level to a Loki label for efficient stream selectors
            // like {level="error"}. Low cardinality (6 values).
            stage.labels {
              values = {
                level = "",
              }
            }

            // Attach msg and module as structured metadata, searchable in
            // LogQL (e.g. | module = "readability") without increasing label
            // cardinality.
            stage.structured_metadata {
              values = {
                msg    = "",
                module = "",
              }
            }

            forward_to = [loki.write.default.receiver]
          }

          // Stage 5: Push labeled log lines to Loki
          loki.write "default" {
            endpoint {
              url = "http://loki.monitoring.svc:3100/loki/api/v1/push"
            }
          }

          // ── Plex file-based log collection ──────────────────────────────
          // Plex writes logs to files rather than stdout. The host directory
          // is mounted into Alloy at /mnt/plex-logs via a hostPath volume.
          // DirectoryOrCreate ensures Alloy pods on nodes without the Plex
          // data directory start with an empty dir (no files to tail).

          // Discover *.log files under the mounted Plex log directory
          local.file_match "plex_logs" {
            path_targets = [{
              __path__     = "/mnt/plex-logs/*.log",
              namespace    = "plex",
              app          = "plex",
              source       = "file",
              service_name = "plex/plex",
            }]
          }

          // Tail discovered Plex log files, starting from end to avoid
          // ingesting all historical data on first startup
          loki.source.file "plex_logs" {
            targets    = local.file_match.plex_logs.targets
            forward_to = [loki.process.plex.receiver]

            tail_from_end = true
          }

          // Parse Plex log format: "Jan 27, 2026 12:39:48.421 [tid] LEVEL - msg"
          loki.process "plex" {
            // Normalize level to lowercase in the raw log line BEFORE regex
            // extraction. stage.replace with source= doesn't modify extracted
            // values in this Alloy version, so we rewrite the log line first.
            stage.replace {
              expression = "\\] INFO - "
              replace    = "] info - "
            }
            stage.replace {
              expression = "\\] DEBUG - "
              replace    = "] debug - "
            }
            stage.replace {
              expression = "\\] WARN(ING)? - "
              replace    = "] warn - "
            }
            stage.replace {
              expression = "\\] ERROR - "
              replace    = "] error - "
            }
            stage.replace {
              expression = "\\] FATAL - "
              replace    = "] fatal - "
            }

            stage.regex {
              expression = "^\\S+ \\d+, \\d{4} [\\d:.]+ \\[\\d+\\] (?P<level>\\S+) - (?P<message>.*)$"
            }

            // Use the parsed message as the log line body
            stage.output {
              source = "message"
            }

            // Promote level to a Loki label
            stage.labels {
              values = {
                level = "",
              }
            }

            // Attach filename as structured metadata so individual log files
            // can be distinguished in LogQL queries
            stage.structured_metadata {
              values = {
                filename = "",
              }
            }

            forward_to = [loki.write.default.receiver]
          }

    controller:
      type: daemonset
      volumes:
        extra:
          - name: plex-logs
            hostPath:
              path: "/mnt/thedatapool/app-data/plex/config/Library/Application Support/Plex Media Server/Logs"
              type: DirectoryOrCreate

    serviceAccount:
      create: true

    rbac:
      create: true

    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
